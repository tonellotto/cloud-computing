{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Installation and Testing\n",
    "\n",
    "> **WARNING: Be careful, the execution of this notebook can compromise your virtual machines. Do not execute any cell twice: please start from the very first cell if you have problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load commands from `commands.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ssh(host, *command):\n",
    "    \"\"\"\n",
    "    :param host: ip address\n",
    "    :type f: string\n",
    "    :param command: command to execute\n",
    "    :type command: string\n",
    "\n",
    "    Execute with SSH the commands on host as the 'hadoop' user, displaying information\n",
    "    \"\"\"\n",
    "    print('===== \\x1b[31m' + 'Started on ' + host + '\\x1b[0m =====')\n",
    "    for cmd in command:\n",
    "        print(cmd)\n",
    "        !ssh hadoop@{host} {cmd}\n",
    "    print('===== \\x1b[31m' + 'Completed on ' + host + '\\x1b[0m =====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally Hadoop can be run in three modes.\n",
    "\n",
    "1. **Standalone (or local) mode**: There are no daemons used in this mode. Hadoop uses the local file system as a substitute for HDFS file system. The jobs will run as if there is 1 mapper and 1 reducer.\n",
    "2. **Pseudo-distributed mode**: All the daemons run on a single machine and this setting mimics the behavior of a cluster. All the daemons run on your machine locally using the HDFS protocol. There can be multiple mappers and reducers.\n",
    "3. **Fully-distributed mode**: This is how Hadoop runs on a real cluster.\n",
    "\n",
    "In these notes we will describe how to set up an [Hadoop 3](https://hadoop.apache.org) installation to work with.\n",
    "We will set up a *fully-distributed cluster* on your assigned virtual machines.\n",
    "\n",
    "The core of Hadoop is composed by two main subsystem:\n",
    "\n",
    "* the **Hadoop Distributed File System** (HDFS), responsible for the distributed data management\n",
    "* the **Yet Another Resource Negotiator** (YARN), responsible for the distributed code execution\n",
    "\n",
    "Both subsystems are implemented according to the **master-workers** architecture.\n",
    "\n",
    "![Master-workers Architecture](https://raw.githubusercontent.com/mesham/epython/master/docs/masterworker.png)\n",
    "\n",
    "Both HDFS and YARN have their own terminology for master and worker nodes.\n",
    "\n",
    "|    | Master         | Worker      |\n",
    "|:--:|:--------------:|:-----------:|\n",
    "|HDFS| NameNode       | DataNode    |\n",
    "|YARN| ResouceManager | NodeManager |\n",
    "\n",
    "While the masters of HDFS and YARN can, in principle, be located on different machines, we will install the HDFS and YARN masters on a single machine, and install the HDFS and YARN workers on all machines (*including the machine hosting the masters).\n",
    "\n",
    "This notebook contains the steps necessary to set up and configure correctly Hadoop on our virtual machines.\n",
    "In particular:\n",
    "\n",
    "1. We will [download & install](#download) Hadoop on all our virtual machines.\n",
    "2. We will [configure](#namenode) a virtual machine to host the HDFS and YARN masters and workers.\n",
    "3. We will [configure](#datanode) the remaining virtual machines to host the HDFS and YARN workers.\n",
    "4. We will [test](#test) your newly install Hadoop cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites <a name=\"prereq\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Populate the following dictionary with the IP addresses (as keys) and the hostnames (as values) of all your virtual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:11.986338Z",
     "start_time": "2020-04-07T21:55:11.980489Z"
    }
   },
   "outputs": [],
   "source": [
    "VMS = {'172.16.3.79': 'datanode1',\n",
    "       '172.16.3.80': 'datanode2', \n",
    "       '172.16.3.81': 'namenode'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Populate the following variable with the IP address of the virtual machine with the namenode role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:54:14.685117Z",
     "start_time": "2020-04-07T21:54:14.673997Z"
    }
   },
   "outputs": [],
   "source": [
    "NAMENODE_IP = '172.16.3.81'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Populate the following variable with the IP address of the remaining virtual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMAINING_IPS = [\n",
    "    '172.16.3.79',\n",
    "    '172.16.3.80'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and install Hadoop <a name=\"download\"/>\n",
    "---\n",
    "**a.** Download [hadoop-3.1.3.tar.gz](https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz) in your home folder on your virtual machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T19:44:28.207459Z",
     "start_time": "2020-04-07T19:43:47.090004Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.3.79\u001b[0m =====\n",
      "wget --progress=bar:force -c -O /home/hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "--2021-04-13 17:31:36--  https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 338075860 (322M) [application/x-gzip]\n",
      "Saving to: ‘/home/hadoop/hadoop.tar.gz’\n",
      "\n",
      "/home/hadoop/hadoop 100%[===================>] 322.41M  13.4MB/s    in 14s     \n",
      "\n",
      "2021-04-13 17:31:49 (23.6 MB/s) - ‘/home/hadoop/hadoop.tar.gz’ saved [338075860/338075860]\n",
      "\n",
      "===== \u001b[31mCompleted on 172.16.3.79\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.80\u001b[0m =====\n",
      "wget --progress=bar:force -c -O /home/hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "--2021-04-13 17:32:05--  https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 338075860 (322M) [application/x-gzip]\n",
      "Saving to: ‘/home/hadoop/hadoop.tar.gz’\n",
      "\n",
      "/home/hadoop/hadoop 100%[===================>] 322.41M  27.4MB/s    in 17s     \n",
      "\n",
      "2021-04-13 17:32:22 (18.9 MB/s) - ‘/home/hadoop/hadoop.tar.gz’ saved [338075860/338075860]\n",
      "\n",
      "===== \u001b[31mCompleted on 172.16.3.80\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.81\u001b[0m =====\n",
      "wget --progress=bar:force -c -O /home/hadoop/hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "--2021-04-13 17:32:33--  https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz\n",
      "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
      "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 338075860 (322M) [application/x-gzip]\n",
      "Saving to: ‘/home/hadoop/hadoop.tar.gz’\n",
      "\n",
      "/home/hadoop/hadoop 100%[===================>] 322.41M  59.0MB/s    in 5.9s    \n",
      "\n",
      "2021-04-13 17:32:39 (54.9 MB/s) - ‘/home/hadoop/hadoop.tar.gz’ saved [338075860/338075860]\n",
      "\n",
      "===== \u001b[31mCompleted on 172.16.3.81\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, commands.WGET_CMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Decompress the Hadoop package you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:22.045210Z",
     "start_time": "2020-04-07T21:55:14.894132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.3.79\u001b[0m =====\n",
      "tar -xvf hadoop.tar.gz --directory=/opt/hadoop --exclude=hadoop-3.1.0/share/doc --strip 1 > /dev/null\n",
      "rm /home/hadoop/hadoop.tar.gz\n",
      "===== \u001b[31mCompleted on 172.16.3.79\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.80\u001b[0m =====\n",
      "tar -xvf hadoop.tar.gz --directory=/opt/hadoop --exclude=hadoop-3.1.0/share/doc --strip 1 > /dev/null\n",
      "rm /home/hadoop/hadoop.tar.gz\n",
      "===== \u001b[31mCompleted on 172.16.3.80\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.81\u001b[0m =====\n",
      "tar -xvf hadoop.tar.gz --directory=/opt/hadoop --exclude=hadoop-3.1.0/share/doc --strip 1 > /dev/null\n",
      "rm /home/hadoop/hadoop.tar.gz\n",
      "===== \u001b[31mCompleted on 172.16.3.81\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, commands.TAR_CMD, commands.RM_CMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** There are environment settings that will be used by Hadoop. The following commands append the correct environment variables to your `/home/hadoop/.bashrc` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:32.077891Z",
     "start_time": "2020-04-07T21:55:28.999693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cazzo\n",
      "cazzo\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    !ssh hadoop@{host} \"sed -i '1,10 s/^/#/' ~/.bashrc\"\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_bashrc()} >> ~/.bashrc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** The following commands check Hadoop installation (you should see no errors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T21:55:37.983835Z",
     "start_time": "2020-04-07T21:55:33.556465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.3.79\u001b[0m =====\n",
      "hadoop version\n",
      "Hadoop 3.1.3\n",
      "Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579\n",
      "Compiled by ztang on 2019-09-12T02:47Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum ec785077c385118ac91aadde5ec9799\n",
      "This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar\n",
      "===== \u001b[31mCompleted on 172.16.3.79\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.80\u001b[0m =====\n",
      "hadoop version\n",
      "cazzo\n",
      "Hadoop 3.1.3\n",
      "Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579\n",
      "Compiled by ztang on 2019-09-12T02:47Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum ec785077c385118ac91aadde5ec9799\n",
      "This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar\n",
      "===== \u001b[31mCompleted on 172.16.3.80\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.81\u001b[0m =====\n",
      "hadoop version\n",
      "Hadoop 3.1.3\n",
      "Source code repository https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579\n",
      "Compiled by ztang on 2019-09-12T02:47Z\n",
      "Compiled with protoc 2.5.0\n",
      "From source with checksum ec785077c385118ac91aadde5ec9799\n",
      "This command was run using /opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar\n",
      "===== \u001b[31mCompleted on 172.16.3.81\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, 'hadoop version')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure the `hadoop-namenode` machine <a name=\"namenode\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Update the `core-site.xml` file located at `/opt/hadoop/etc/hadoop/` to define the name node URI on this machine.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop-namenode:9820/</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:07:53.169774Z",
     "start_time": "2020-04-07T22:07:52.007599Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_core_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/core-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Update the `hdfs-site.xml` file located at `/opt/hadoop/etc/hadoop/` to define the path on the local filesystem where the name node stores the namespace and transactions logs persistently and to configure the HDFS subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>file:///opt/hdfs/namenode</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>file:///opt/hdfs/datanode</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.permissions</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.use.datanode.hostname</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:10:07.811788Z",
     "start_time": "2020-04-07T22:10:03.228836Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_hdfs_site()} > /opt/hadoop/etc/hadoop/hdfs-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Update the `yarn-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the YARN subsystem.\n",
    "The file must contain the following lines.\n",
    "\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>{hadoop_namenode}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.local-dirs</name>\n",
    "    <value>file:///opt/yarn/local</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.log-dirs</name>\n",
    "    <value>file:///opt/yarn/logs</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:11:08.241162Z",
     "start_time": "2020-04-07T22:11:07.094038Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_yarn_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/yarn-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Update the `mapred-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the MAPREDUCE subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.address</name>\n",
    "    <value>{namenode_hostname}:10020</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.webapp.address</name>\n",
    "    <value>{namenode_hostname}:19888</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.intermediate-done-dir</name>\n",
    "    <value>/mr-history/tmp</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.jobhistory.done-dir</name>\n",
    "    <value>/mr-history/done</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:11:10.387867Z",
     "start_time": "2020-04-07T22:11:09.326366Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_namenode_mapred_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/mapred-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If your machines have more than 2 GB of RAM or if you are interested in the numbers we specified in the YARN and MAPRED configuration files, check the Append A on the [Hadoop Memory Allocaltion](#memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e.** Update the `workers` file located in `/opt/hadoop/etc/hadoop` to define the MAPREDUCE workers.\n",
    "With our virtual machines listed [here](#prereq), the file must contain the following lines.\n",
    "```\n",
    "172.16.0.225\n",
    "172.16.0.221\n",
    "172.16.0.224\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T22:17:40.521472Z",
     "start_time": "2020-04-07T22:17:39.217530Z"
    }
   },
   "outputs": [],
   "source": [
    "!ssh hadoop@{NAMENODE_IP} 'printf \"%s\\n\" {commands.get_workers(VMS)} > /opt/hadoop/etc/hadoop/workers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure the `hadoop-datanode` machines <a name=\"datanode\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a.** Update the `core-site.xml` file located at `/opt/hadoop/etc/hadoop/` to define the name node URI on thie other datanodes.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://hadoop-namenode:9820/</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_core_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/core-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Update the `hdfs-site.xml` file located at `/opt/hadoop/etc/hadoop/` to configure the HDFS subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>file:///opt/hdfs/datanode</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>2</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.permissions</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>dfs.datanode.use.datanode.hostname</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_hdfs_site()} > /opt/hadoop/etc/hadoop/hdfs-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** Update the `yarn-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the YARN subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.aux-services</name>\n",
    "    <value>mapreduce_shuffle</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.resourcemanager.hostname</name>\n",
    "    <value>{namenode_hostname}</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
    "    <value>1536</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
    "    <value>128</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "    <value>false</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_yarn_site(VMS[NAMENODE_IP])} > /opt/hadoop/etc/hadoop/yarn-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d.** Update the `mapred-site.xml` file located at `/opt/hadoop/etc/hadoop` to configure the MAPREDUCE subsystem.\n",
    "The file must contain the following lines.\n",
    "```xml\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>mapreduce.framework.name</name>\n",
    "    <value>yarn</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.env</name>\n",
    "    <value>HADOOP_MAPRED_HOME=/opt/hadoop</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>yarn.app.mapreduce.am.resource.mb</name>\n",
    "    <value>512</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.map.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>mapreduce.reduce.memory.mb</name>\n",
    "    <value>256</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "The following command updates the file automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for host in REMAINING_IPS:\n",
    "    !ssh hadoop@{host} 'printf \"%s\\n\" {commands.get_datanode_mapred_site()} > /opt/hadoop/etc/hadoop/mapred-site.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If your machines have more than 2 GB of RAM or if you are interested in the numbers we specified in the YARN and MAPRED configuration files, check the Append A on the [Hadoop Memory Allocaltion](#memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a summary, please double check the content of the files list in the following picture, on the cluster machines:\n",
    "\n",
    "![Hadoop Configuration Files](hadoop_conf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start, test and stop Hadoop <a name=\"test\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From now, all commands will be issued from the `hadoop-namenode` virtual machine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a**. Delete the contents of the local HDFS file system.\n",
    "Note: **This causes the loss of all information stored in HDFS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.3.79\u001b[0m =====\n",
      "rm -rf /opt/hdfs/namenode/*\n",
      "zsh:1: no matches found: /opt/hdfs/namenode/*\n",
      "===== \u001b[31mCompleted on 172.16.3.79\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.79\u001b[0m =====\n",
      "rm -rf /opt/hdfs/datanode/*\n",
      "zsh:1: no matches found: /opt/hdfs/datanode/*\n",
      "===== \u001b[31mCompleted on 172.16.3.79\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.80\u001b[0m =====\n",
      "rm -rf /opt/hdfs/namenode/*\n",
      "zsh:1: no matches found: /opt/hdfs/namenode/*\n",
      "===== \u001b[31mCompleted on 172.16.3.80\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.80\u001b[0m =====\n",
      "rm -rf /opt/hdfs/datanode/*\n",
      "zsh:1: no matches found: /opt/hdfs/datanode/*\n",
      "===== \u001b[31mCompleted on 172.16.3.80\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.81\u001b[0m =====\n",
      "rm -rf /opt/hdfs/namenode/*\n",
      "zsh:1: no matches found: /opt/hdfs/namenode/*\n",
      "===== \u001b[31mCompleted on 172.16.3.81\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.81\u001b[0m =====\n",
      "rm -rf /opt/hdfs/datanode/*\n",
      "zsh:1: no matches found: /opt/hdfs/datanode/*\n",
      "===== \u001b[31mCompleted on 172.16.3.81\u001b[0m =====\n",
      "Stopping namenodes on [namenode]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [namenode]\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, 'rm -rf /opt/hdfs/namenode/*')\n",
    "    run_ssh(host, 'rm -rf /opt/hdfs/datanode/*')\n",
    "    \n",
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'stop-dfs.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b**. Format the HDFS filesystem at the namenode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-13 18:08:00,651 INFO namenode.NameNode: STARTUP_MSG: \r\n",
      "/************************************************************\r\n",
      "STARTUP_MSG: Starting NameNode\r\n",
      "STARTUP_MSG:   host = namenode/172.16.3.81\r\n",
      "STARTUP_MSG:   args = [-format, -force]\r\n",
      "STARTUP_MSG:   version = 3.1.3\r\n",
      "STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/opt/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.4.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.1.3.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.18.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.1.3.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.7.8.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.1.3.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.1.3-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.1.3.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.5.jar:/opt/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.3-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.1.3.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.1.3.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.1.3.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/opt/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-1.0.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.1.3.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.1.3.jar\r\n",
      "STARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r ba631c436b806728f8ec2f54ab1e289526c90579; compiled by 'ztang' on 2019-09-12T02:47Z\r\n",
      "STARTUP_MSG:   java = 1.8.0_282\r\n",
      "************************************************************/\r\n",
      "2021-04-13 18:08:00,658 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-13 18:08:00,761 INFO namenode.NameNode: createNameNode [-format, -force]\n",
      "Formatting using clusterid: CID-dcfa04bc-2017-40b8-9a87-522b499cdedc\n",
      "2021-04-13 18:08:01,235 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "2021-04-13 18:08:01,266 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "2021-04-13 18:08:01,268 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "2021-04-13 18:08:01,269 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "2021-04-13 18:08:01,273 INFO namenode.FSNamesystem: fsOwner             = hadoop (auth:SIMPLE)\n",
      "2021-04-13 18:08:01,273 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "2021-04-13 18:08:01,273 INFO namenode.FSNamesystem: isPermissionEnabled = false\n",
      "2021-04-13 18:08:01,274 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "2021-04-13 18:08:01,317 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "2021-04-13 18:08:01,328 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "2021-04-13 18:08:01,328 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "2021-04-13 18:08:01,332 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "2021-04-13 18:08:01,333 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Apr 13 18:08:01\n",
      "2021-04-13 18:08:01,334 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "2021-04-13 18:08:01,334 INFO util.GSet: VM type       = 64-bit\n",
      "2021-04-13 18:08:01,336 INFO util.GSet: 2.0% max memory 420 MB = 8.4 MB\n",
      "2021-04-13 18:08:01,336 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "2021-04-13 18:08:01,342 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
      "2021-04-13 18:08:01,349 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "2021-04-13 18:08:01,349 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "2021-04-13 18:08:01,350 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "2021-04-13 18:08:01,350 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "2021-04-13 18:08:01,350 INFO blockmanagement.BlockManager: defaultReplication         = 2\n",
      "2021-04-13 18:08:01,350 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "2021-04-13 18:08:01,350 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "2021-04-13 18:08:01,351 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "2021-04-13 18:08:01,351 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
      "2021-04-13 18:08:01,351 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "2021-04-13 18:08:01,351 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "2021-04-13 18:08:01,371 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
      "2021-04-13 18:08:01,390 INFO util.GSet: Computing capacity for map INodeMap\n",
      "2021-04-13 18:08:01,390 INFO util.GSet: VM type       = 64-bit\n",
      "2021-04-13 18:08:01,391 INFO util.GSet: 1.0% max memory 420 MB = 4.2 MB\n",
      "2021-04-13 18:08:01,391 INFO util.GSet: capacity      = 2^19 = 524288 entries\n",
      "2021-04-13 18:08:01,391 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "2021-04-13 18:08:01,391 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
      "2021-04-13 18:08:01,391 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "2021-04-13 18:08:01,391 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "2021-04-13 18:08:01,396 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
      "2021-04-13 18:08:01,401 INFO snapshot.SnapshotManager: SkipList is disabled\n",
      "2021-04-13 18:08:01,405 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "2021-04-13 18:08:01,405 INFO util.GSet: VM type       = 64-bit\n",
      "2021-04-13 18:08:01,405 INFO util.GSet: 0.25% max memory 420 MB = 1.0 MB\n",
      "2021-04-13 18:08:01,405 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
      "2021-04-13 18:08:01,426 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "2021-04-13 18:08:01,426 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "2021-04-13 18:08:01,426 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "2021-04-13 18:08:01,429 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "2021-04-13 18:08:01,429 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "2021-04-13 18:08:01,431 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "2021-04-13 18:08:01,431 INFO util.GSet: VM type       = 64-bit\n",
      "2021-04-13 18:08:01,431 INFO util.GSet: 0.029999999329447746% max memory 420 MB = 129.0 KB\n",
      "2021-04-13 18:08:01,431 INFO util.GSet: capacity      = 2^14 = 16384 entries\n",
      "Data exists in Storage Directory root= /opt/hdfs/namenode; location= null. Formatting anyway.\n",
      "2021-04-13 18:08:01,460 INFO namenode.FSImage: Allocated new BlockPoolId: BP-572541278-172.16.3.81-1618333681452\n",
      "2021-04-13 18:08:01,460 INFO common.Storage: Will remove files: [/opt/hdfs/namenode/current/fsimage_0000000000000000000, /opt/hdfs/namenode/current/seen_txid, /opt/hdfs/namenode/current/fsimage_0000000000000000000.md5, /opt/hdfs/namenode/current/edits_inprogress_0000000000000000001, /opt/hdfs/namenode/current/VERSION]\n",
      "2021-04-13 18:08:01,524 INFO common.Storage: Storage directory /opt/hdfs/namenode has been successfully formatted.\n",
      "2021-04-13 18:08:01,557 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "2021-04-13 18:08:01,640 INFO namenode.FSImageFormatProtobuf: Image file /opt/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 390 bytes saved in 0 seconds .\n",
      "2021-04-13 18:08:01,677 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "2021-04-13 18:08:01,683 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\n",
      "2021-04-13 18:08:01,683 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at namenode/172.16.3.81\n",
      "************************************************************/\n"
     ]
    }
   ],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'hdfs namenode -format -force'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c**. Creating the HDFS home folder for the `hadoop` user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [namenode]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [namenode]\n",
      "Stopping namenodes on [namenode]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [namenode]\n"
     ]
    }
   ],
   "source": [
    "!ssh hadoop@{host} 'start-dfs.sh'\n",
    "!ssh hadoop@{host} 'hadoop fs -mkdir -p /user/hadoop'\n",
    "!ssh hadoop@{host} 'stop-dfs.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d**. Starting HDFS & YARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [namenode]\n",
      "Starting datanodes\n",
      "Starting secondary namenodes [namenode]\n",
      "Starting resourcemanager\n",
      "Starting nodemanagers\n"
     ]
    }
   ],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'start-dfs.sh'\n",
    "!ssh hadoop@{host} 'start-yarn.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e**. Checking all daemons up and running. You should receive 5 lines from the namenode machine and 3 lines from the datanote machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== \u001b[31mStarted on 172.16.3.79\u001b[0m =====\n",
      "jps\n",
      "17698 DataNode\n",
      "18130 Jps\n",
      "17930 NodeManager\n",
      "===== \u001b[31mCompleted on 172.16.3.79\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.80\u001b[0m =====\n",
      "jps\n",
      "13830 Jps\n",
      "13629 NodeManager\n",
      "13437 DataNode\n",
      "===== \u001b[31mCompleted on 172.16.3.80\u001b[0m =====\n",
      "===== \u001b[31mStarted on 172.16.3.81\u001b[0m =====\n",
      "jps\n",
      "25249 NodeManager\n",
      "24050 NameNode\n",
      "25494 Jps\n",
      "24537 SecondaryNameNode\n",
      "24860 ResourceManager\n",
      "===== \u001b[31mCompleted on 172.16.3.81\u001b[0m =====\n"
     ]
    }
   ],
   "source": [
    "for host in VMS:\n",
    "    run_ssh(host, 'jps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check logs at `/opt/hadoop/logs` on the 3 machines and check if everything is alright, or running the `hdfs dfsadmin -report` command (it must return `Live datanodes (3)`).\n",
    "\n",
    "You can access Hadoop on a browser on your local machine (use IP addresses, not hostnames):\n",
    "- HDFS subsystem: `http://172.16.3.81:9870/`\n",
    "- YARN subsystem: `http://172.16.0.1:8088/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f.** Run an example provided by Hadoop.\n",
    "* Wait a minute before running, the daemons should perform some initialization steps\n",
    "* Ignore initial errors `No such file or directory`\n",
    "* Ignore logger message by logger `sasl.SaslDataTransferClient`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `input': No such file or directory\n",
      "rm: `output': No such file or directory\n",
      "2021-04-14 08:35:04,492 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:05,566 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:05,701 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:05,768 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:05,870 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:05,936 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,062 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,121 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,185 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,255 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,312 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,390 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,452 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,516 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,590 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,660 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,745 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,817 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:06,902 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,007 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,085 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,156 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,220 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,305 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,374 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,438 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,492 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,574 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,656 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,718 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,773 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:07,844 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:10,512 INFO client.RMProxy: Connecting to ResourceManager at namenode/172.16.3.81:8032\n",
      "2021-04-14 08:35:11,005 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1618385672230_0001\n",
      "2021-04-14 08:35:11,133 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:11,335 INFO input.FileInputFormat: Total input files to process : 9\n",
      "2021-04-14 08:35:11,455 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:11,519 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:11,538 INFO mapreduce.JobSubmitter: number of splits:9\n",
      "2021-04-14 08:35:11,734 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:11,771 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1618385672230_0001\n",
      "2021-04-14 08:35:11,771 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-04-14 08:35:12,016 INFO conf.Configuration: resource-types.xml not found\n",
      "2021-04-14 08:35:12,016 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2021-04-14 08:35:12,413 INFO impl.YarnClientImpl: Submitted application application_1618385672230_0001\n",
      "2021-04-14 08:35:12,459 INFO mapreduce.Job: The url to track the job: http://namenode:8088/proxy/application_1618385672230_0001/\n",
      "2021-04-14 08:35:12,459 INFO mapreduce.Job: Running job: job_1618385672230_0001\n",
      "2021-04-14 08:35:19,619 INFO mapreduce.Job: Job job_1618385672230_0001 running in uber mode : false\n",
      "2021-04-14 08:35:19,620 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-04-14 08:35:31,723 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2021-04-14 08:35:41,774 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-04-14 08:35:43,782 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-04-14 08:35:45,802 INFO mapreduce.Job: Job job_1618385672230_0001 completed successfully\n",
      "2021-04-14 08:35:45,910 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=186\n",
      "\t\tFILE: Number of bytes written=2181853\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=28478\n",
      "\t\tHDFS: Number of bytes written=302\n",
      "\t\tHDFS: Number of read operations=32\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=9\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=9\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=286072\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20234\n",
      "\t\tTotal time spent by all map tasks (ms)=143036\n",
      "\t\tTotal time spent by all reduce tasks (ms)=10117\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=143036\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=10117\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=36617216\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2589952\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=777\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=168\n",
      "\t\tMap output materialized bytes=234\n",
      "\t\tInput split bytes=1068\n",
      "\t\tCombine input records=6\n",
      "\t\tCombine output records=6\n",
      "\t\tReduce input groups=6\n",
      "\t\tReduce shuffle bytes=234\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =9\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=9\n",
      "\t\tGC time elapsed (ms)=2611\n",
      "\t\tCPU time spent (ms)=5350\n",
      "\t\tPhysical memory (bytes) snapshot=2305642496\n",
      "\t\tVirtual memory (bytes) snapshot=19171221504\n",
      "\t\tTotal committed heap usage (bytes)=1706557440\n",
      "\t\tPeak Map Physical memory (bytes)=271491072\n",
      "\t\tPeak Map Virtual memory (bytes)=1917648896\n",
      "\t\tPeak Reduce Physical memory (bytes)=172666880\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1926463488\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=27410\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-14 08:35:45,945 INFO client.RMProxy: Connecting to ResourceManager at namenode/172.16.3.81:8032\n",
      "2021-04-14 08:35:46,033 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hadoop/.staging/job_1618385672230_0002\n",
      "2021-04-14 08:35:46,079 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:46,188 INFO input.FileInputFormat: Total input files to process : 1\n",
      "2021-04-14 08:35:46,241 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:46,364 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:46,408 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2021-04-14 08:35:46,468 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2021-04-14 08:35:46,524 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1618385672230_0002\n",
      "2021-04-14 08:35:46,524 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2021-04-14 08:35:46,548 INFO impl.YarnClientImpl: Submitted application application_1618385672230_0002\n",
      "2021-04-14 08:35:46,553 INFO mapreduce.Job: The url to track the job: http://namenode:8088/proxy/application_1618385672230_0002/\n",
      "2021-04-14 08:35:46,554 INFO mapreduce.Job: Running job: job_1618385672230_0002\n",
      "2021-04-14 08:36:04,712 INFO mapreduce.Job: Job job_1618385672230_0002 running in uber mode : false\n",
      "2021-04-14 08:36:04,713 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2021-04-14 08:36:08,753 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2021-04-14 08:36:15,809 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2021-04-14 08:36:16,824 INFO mapreduce.Job: Job job_1618385672230_0002 completed successfully\n",
      "2021-04-14 08:36:16,879 INFO mapreduce.Job: Counters: 53\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=186\n",
      "\t\tFILE: Number of bytes written=435463\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=433\n",
      "\t\tHDFS: Number of bytes written=132\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4310\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7990\n",
      "\t\tTotal time spent by all map tasks (ms)=2155\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3995\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2155\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3995\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=551680\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1022720\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6\n",
      "\t\tMap output records=6\n",
      "\t\tMap output bytes=168\n",
      "\t\tMap output materialized bytes=186\n",
      "\t\tInput split bytes=131\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=186\n",
      "\t\tReduce input records=6\n",
      "\t\tReduce output records=6\n",
      "\t\tSpilled Records=12\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=124\n",
      "\t\tCPU time spent (ms)=1070\n",
      "\t\tPhysical memory (bytes) snapshot=440197120\n",
      "\t\tVirtual memory (bytes) snapshot=3838574592\n",
      "\t\tTotal committed heap usage (bytes)=290979840\n",
      "\t\tPeak Map Physical memory (bytes)=269750272\n",
      "\t\tPeak Map Virtual memory (bytes)=1915076608\n",
      "\t\tPeak Reduce Physical memory (bytes)=170446848\n",
      "\t\tPeak Reduce Virtual memory (bytes)=1923497984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=302\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=132\n",
      "2021-04-14 08:36:19,505 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "1\tdfsadmin\n",
      "1\tdfs.replication\n",
      "1\tdfs.permissions\n",
      "1\tdfs.namenode.name.dir\n",
      "1\tdfs.datanode.use.datanode.hostname\n",
      "1\tdfs.datanode.data.dir\n"
     ]
    }
   ],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'hadoop fs -rm -r input output'\n",
    "!ssh hadoop@{host} 'hadoop fs -put /opt/hadoop/etc/hadoop/ input'\n",
    "!ssh hadoop@{host} \"hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar grep /user/hadoop/input/*.xml /user/hadoop/output 'dfs[a-z.]+'\"\n",
    "!ssh hadoop@{host} 'hadoop fs -cat output/part-r-00000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g.** Stop HDFS & YARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [namenode]\n",
      "Stopping datanodes\n",
      "Stopping secondary namenodes [namenode]\n",
      "Stopping nodemanagers\n",
      "172.16.3.80: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9\n",
      "172.16.3.81: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9\n",
      "172.16.3.79: WARNING: nodemanager did not stop gracefully after 5 seconds: Trying to kill with kill -9\n",
      "Stopping resourcemanager\n"
     ]
    }
   ],
   "source": [
    "host = NAMENODE_IP\n",
    "!ssh hadoop@{host} 'stop-dfs.sh'\n",
    "!ssh hadoop@{host} 'stop-yarn.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "If you get an error like the following:\n",
    "```bash\n",
    "[2020-01-14 08:48:28.567]Container [pid=155967,containerID=container_1578991625193_0002_01_000023] is running 380426752B beyond the 'VIRTUAL' memory limit. Current usage: 151.6 MB of 1 GB physical memory used; 2.5 GB of 2.1 GB virtual memory used. Killing container.\n",
    "```\n",
    "you are using more virtual memory than your current limit of 2.1 Gb. This can be resolved in two ways:\n",
    "\n",
    "  1. **Disable Virtual Memory Limit Checking**<br>YARN will simply ignore the limit; in order to do this, add this to your `yarn-site.xml` _on each machine_:\n",
    "      ```bash\n",
    "      <property>\n",
    "        <name>yarn.nodemanager.vmem-check-enabled</name>\n",
    "        <value>false</value>\n",
    "      </property>\n",
    "      ```\n",
    "      The default for this setting is `true`.\n",
    "\n",
    "  2. **Increase Virtual Memory to Physical Memory Ratio**<br>In your `yarn-site.xml` change this to a higher value than is currently set, _on each machine_:\n",
    "      ```bash\n",
    "      <property>\n",
    "        <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
    "        <value>5</value>\n",
    "      </property>\n",
    "      ```\n",
    "      The default is 2.1.<br>\n",
    "      You could also increase the amount of physical memory you allocate to a container.<br>\n",
    "      _Make sure you don't forget to restart yarn after you change the configuration_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A. Understanding the Hadoop Memory Allocation <a name=\"memory\"/>\n",
    "\n",
    "Memory allocation can be complex on low RAM nodes because default values are not suitable for nodes with less than 8 GB of RAM.\n",
    "In this section we highlight how memory allocation works for MapReduce jobs, and provide a sample configuration for 2 GB RAM nodes.\n",
    "\n",
    "A YARN job is executed with two kind of resources:\n",
    "\n",
    "* An **application master** (AM), which is responsible for monitoring the application and coordinating distributed executors in the cluster.\n",
    "* Some **executors**, that are created by the AM to actually run the job. For a MapReduce job, they will perform map or reduce operation, in parallel.\n",
    "\n",
    "Both are run in **containers** on **worker nodes**. Each worker node runs a **NodeManager** daemon that is responsible for container creation on the node.\n",
    "The whole cluster is managed by a **ResourceManager** that schedules container allocation on all the worker nodes, depending on capacity requirements and current charge.\n",
    "\n",
    "Four types of resource allocations need to be configured properly for the cluster to work. These are:\n",
    "\n",
    "1. _How much memory can be allocated for YARN containers on a single node._   This limit should be higher than all the others; otherwise, container allocation will be rejected and applications will fail. However, it should not be the entire amount of RAM on the node.\n",
    "This value is configured in the `yarn-site.xml` file with the `yarn.nodemanager.resource.memory-mb` property.\n",
    "\n",
    "2. _How much memory a single container can consume and the minimum memory allocation allowed._\n",
    "A container will never be bigger than the maximum, or else allocation will fail and will always be allocated as a multiple of the minimum amount of RAM.\n",
    "Those values are configured in the `yarn-site.xml` file with the `yarn.scheduler.maximum-allocation-mb` and `yarn.scheduler.minimum-allocation-mb` properties.\n",
    "\n",
    "3. _How much memory will be allocated to the ApplicationMaster._\n",
    "This is a constant value that should fit in the container maximum size.\n",
    "This value is configured in the `mapred-site.xml` with the `yarn.app.mapreduce.am.resource.mb` property.\n",
    "\n",
    "4. _How much memory will be allocated to each map or reduce operation._\n",
    "This should be less than the maximum size.\n",
    "This value is configured in the `mapred-site.xml` file with the `mapreduce.map.memory.mb` and `mapreduce.reduce.memory.mb` properties.\n",
    "\n",
    "The relationship between all those properties can be seen in the following figure:\n",
    "![](https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/hadoop-2-memory-allocation-new.png)\n",
    "\n",
    "For 2 GB nodes, a working configuration may be:\n",
    "\n",
    "|Property|Value|\n",
    "|:-------|:----|\n",
    "|`yarn.nodemanager.resource.memory-mb`  | 1536|\n",
    "|`yarn.scheduler.maximum-allocation-mb` | 1536|\n",
    "|`yarn.scheduler.minimum-allocation-mb` |  128|\n",
    "|`yarn.app.mapreduce.am.resource.mb`    |  512|\n",
    "|`mapreduce.map.memory.mb`              |  256|\n",
    "|`mapreduce.reduce.memory.mb`           |  256|\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
